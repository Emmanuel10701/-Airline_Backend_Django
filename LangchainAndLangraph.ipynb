{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlDEJD7nD6er9wiiNvKT1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emmanuel10701/-Airline_Backend_Django/blob/main/LangchainAndLangraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iEsevwyAkJWB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5f3d72"
      },
      "source": [
        "# -------------------------------\n",
        "# 1. Install Dependencies\n",
        "# -------------------------------\n",
        "!pip install langchain langchain-community openai\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Import Modules\n",
        "# -------------------------------\n",
        "import os\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Set OpenAI API Key\n",
        "# -------------------------------\n",
        "# ‚ö†Ô∏è For security, this key should be rotated as soon as possible\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define Prompt Template\n",
        "# -------------------------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Hello {name}, welcome to learning LangChain! üöÄ\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Load Model\n",
        "# -------------------------------\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Connect Model + Prompt\n",
        "# -------------------------------\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Run Chain with Input\n",
        "# -------------------------------\n",
        "result = chain.run({\"name\": \"Juma\"})\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edc27535"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain-google-genai langchain-community\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# üîë Set your Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "# üìù Create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in very simple terms for a beginner.\"\n",
        ")\n",
        "\n",
        "# ü§ñ Load Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
        "\n",
        "# üîó Create LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# üöÄ Run the chain with streaming enabled\n",
        "print(\"Gemini Response (Streaming):\\n\")\n",
        "for chunk in chain.stream({\"topic\": \"LangChain and how it works\"}):\n",
        "    # The 'text' key typically holds the generated content in LLMChain stream results\n",
        "    if chunk and 'text' in chunk:\n",
        "        print(chunk['text'], end=\"\", flush=True)\n",
        "print(\"\\n--- Streaming Complete ---\")\n",
        "\n",
        "# The conversational chain and other examples would follow a similar pattern for streaming.\n",
        "# For ConversationChain, you might need to access the underlying LLM or chain components\n",
        "# that support streaming, or wrap it in a way that enables streaming for its responses.\n",
        "\n",
        "# Example for a direct LLM invocation with streaming:\n",
        "print(\"\\nDirect LLM Streaming:\\n\")\n",
        "for chunk in llm.stream(\"Tell me a short story about a brave knight.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print(\"\\n--- Streaming Complete ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langgraph langchain-google-genai\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict\n",
        "\n",
        "# üîë API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "# ü§ñ Load Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
        "\n",
        "# --- Step 1: Define state class ---\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph.\"\"\"\n",
        "    msg: str\n",
        "\n",
        "# --- Step 2: Define a single node for the chat logic ---\n",
        "def chat_node(state: GraphState):\n",
        "    \"\"\"A node to handle the chat logic.\"\"\"\n",
        "    prompt_text = \"Say a warm hello to Juma and explain that LangGraph helps organize AI workflows. Then, encourage Juma to keep learning LangChain and LangGraph.\"\n",
        "    response = llm.invoke(prompt_text)\n",
        "    return {\"msg\": response.content}\n",
        "\n",
        "# --- Step 3: Build the graph ---\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"chat\", chat_node)\n",
        "\n",
        "# Flow: chat ‚Üí END\n",
        "graph.add_edge(\"chat\", END)\n",
        "\n",
        "# Entry point\n",
        "graph.set_entry_point(\"chat\")\n",
        "\n",
        "# --- Step 4: Compile & Run with .invoke() ---\n",
        "app = graph.compile()\n",
        "\n",
        "# Run the graph and get the complete result\n",
        "result = app.invoke({})\n",
        "\n",
        "# ü§ñ Print the full result\n",
        "print(\"ü§ñ Full Gemini Response:\")\n",
        "print(result[\"msg\"])"
      ],
      "metadata": {
        "id": "Raoo02F4Gtlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "330eb705"
      },
      "source": [
        "# Task\n",
        "Modify the provided Python code that uses LangChain and OpenAI to stream the output word by word instead of displaying the full response at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "352947b7"
      },
      "source": [
        "## Identify the relevant cell\n",
        "\n",
        "### Subtask:\n",
        "Determine which code cell needs to be modified to implement streaming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a04f5b"
      },
      "source": [
        "## Modify the model invocation\n",
        "\n",
        "### Subtask:\n",
        "Change the `invoke` method to a streaming method provided by the LangChain model (e.g., `stream`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a23dc4d3"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the code in cell `4f5f3d72` to use the `stream` method instead of `run`. I will replace `chain.run({\"name\": \"Juma\"})` with `chain.stream({\"name\": \"Juma\"})`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf003cee"
      },
      "source": [
        "# -------------------------------\n",
        "# 1. Install Dependencies\n",
        "# -------------------------------\n",
        "!pip install langchain langchain-community openai\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Import Modules\n",
        "# -------------------------------\n",
        "import os\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Set OpenAI API Key\n",
        "# -------------------------------\n",
        "# ‚ö†Ô∏è For security, this key should be rotated as soon as possible\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define Prompt Template\n",
        "# -------------------------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Hello {name}, welcome to learning LangChain! üöÄ\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Load Model\n",
        "# -------------------------------\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Connect Model + Prompt\n",
        "# -------------------------------\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Run Chain with Input\n",
        "# -------------------------------\n",
        "result = chain.stream({\"name\": \"Juma\"})\n",
        "for chunk in result:\n",
        "    print(chunk['text'], end='', flush=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9402a5b"
      },
      "source": [
        "## Modify the model invocation\n",
        "\n",
        "### Subtask:\n",
        "Retry modifying the model invocation to use a streaming method, addressing the previous `RateLimitError`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa11cd77"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to stream failed due to a RateLimitError from the OpenAI API. The instructions for this subtask are to retry modifying the model invocation to use a streaming method, addressing the previous RateLimitError. Since the code modification for streaming was correct, the issue is with the OpenAI API key or quota. I will retry running the code with the streaming modification. If the RateLimitError persists, I will report failure as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "582bfe19"
      },
      "source": [
        "# -------------------------------\n",
        "# 1. Install Dependencies\n",
        "# -------------------------------\n",
        "# !pip install langchain langchain-community openai # Dependencies are already installed\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Import Modules\n",
        "# -------------------------------\n",
        "import os\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Set OpenAI API Key\n",
        "# -------------------------------\n",
        "# ‚ö†Ô∏è For security, this key should be rotated as soon as possible\n",
        "# The API key is already set in the previous cell.\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define Prompt Template\n",
        "# -------------------------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Hello {name}, welcome to learning LangChain! üöÄ\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Load Model\n",
        "# -------------------------------\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Connect Model + Prompt\n",
        "# -------------------------------\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Run Chain with Input (Streaming)\n",
        "# -------------------------------\n",
        "result = chain.stream({\"name\": \"Juma\"})\n",
        "for chunk in result:\n",
        "    print(chunk['text'], end='', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9476f3b3"
      },
      "source": [
        "## Process the streamed output\n",
        "\n",
        "### Subtask:\n",
        "Process the streamed output from the retried model invocation and print it word by word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8893f37"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to stream the output resulted in a `RateLimitError` from the OpenAI API.\n",
        "*   Subsequent attempts to stream the output using `chain.stream()` also resulted in the same `RateLimitError`, preventing the successful demonstration of word-by-word streaming.\n",
        "*   The code modification to use `chain.stream()` and iterate through the chunks to print the text was correctly implemented.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary obstacle to completing the task is the `RateLimitError`. The API key needs to be checked for validity and sufficient quota.\n",
        "*   Once the API issue is resolved, the current code should successfully stream the output word by word as intended.\n"
      ]
    }
  ]
}